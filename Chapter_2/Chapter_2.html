<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>The Simple Regression Model</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding/datatables.js"></script>
    <script src="libs/jquery/jquery-3.6.0.min.js"></script>
    <link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.min.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# The Simple Regression Model
]
.subtitle[
## Introductory Econometrics: A Modern Approach Chapter 2
]

---









---
## 2.1 Definition of the Simple Regression Model

“Explains variable `\(y\)` in terms of variable `\(x\)`”


&lt;img src="21.png" width="400px" /&gt;

---
### The Simple Regression Model

 

By how much does the dependent variable change if the independent variable is increased by one unit?

--

`$$\frac{\Delta y}{\Delta x}=\beta_{1} \quad$$`

--

Interpretation only correct if all other  things remain equal when the independent variable is increased by one unit.

 `$$\quad \frac{\Delta u}{\Delta x}=0$$`

???
add notes
---
### The Simple Regression Model
  - Example: Soybean yield and fertilizer

&lt;img src="22.png" width="600px" /&gt;

--

  - Example: A simple wage equation 

&lt;img src="23.png" width="600px" /&gt;

--

Does `\(\beta_{1}\)` measure a CAUSAL effect of `\(x\)` on `\(y\)`?

--

  - It depends on the relationship with `\(x\)` and `\(u\)` 


---
### The Simple Regression Model

**Assumption 1: `\(E(u)=0\)`** 

As long as `\(\beta_{0}\)` is included in the regression, nothing is lost assuming that the average value of  `\(u\)` in the population is zero. Why?

--

 - When is there a causal interpretation?
 
--

**Assumption 2:  `\(E(u|x)=E(u)\)`**
 
   - The independent variable does not contain information about the mean of the unobserved factors 
   - Average values of the unobservables is the same across all values of `\(x\)`, and that the common average is equal to the average of `\(u\)` over the entire population
 
???
prepare the derivation for including/ not including the intercept

---
### The Simple Regression Model

  - Assumption 1: `\(E(u)=0\)` 
  
  - Assumption 2:  `\(E(u|x)=E(u)\)`

Combining the assumption 1 and 2:

--  
  
**Zero Conditional Mean Assumption:** `\(E(u|x)=0\)`

If zero conditional mean assumption holds, then we have a causal interpretation of our coefficient.

Example:

`$$wage=\beta_{0}+\beta_{1} educ+u$$`
--

Does `\(\widehat{\beta_{1}}\)` have a causal interpretation (e.g. does education causes higher wages)? Why or why not? 

--

e.g. Intelligence is part of the error term `\(u\)`

--

The conditional mean independence assumption is unlikely to hold because individuals with more education will also be more intelligent on average.



---
### Population regression function (PFR)

- The zero conditional mean independence assumption `\(E(u|x)=0\)` implies that:

`$$\begin{aligned}
E(y \mid x) &amp;=E\left(\beta_{0}+\beta_{1} x+u \mid x\right) \\
&amp;=\beta_{0}+\beta_{1} x+E(u \mid x) \\
&amp;=\beta_{0}+\beta_{1} x
\end{aligned}$$`

- It means that the average value of the dependent variable can be expressed as a linear function of the explanatory variable. This is the *unknown* population function.

--

- `\(E(y \mid x)\)` tells us how the average value if `\(y\)` changes with `\(x\)`; it does not say that `\(y\)` equals `\(\beta_{0}+\beta_{1} x\)` for all units in the population
 
--

  - Example `\(E(colGPA|hsGPA)=1.5+ 0.5hsGPA\)`

--

- Note: One unit increase in `\(x\)` changes the *expected values* of  `\(y\)` by the amount of `\(\beta_{1}\)`

???
check notes here for example

---
### Population regression function (PFR)

&lt;img src="25.png" width="700px" style="display: block; margin: auto;" /&gt;

---
### Causation VS Correlation

What is correlation?

- Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate).
- [Spurious correlations LINK](https://www.tylervigen.com/spurious-correlations) 

--

Correlation **DOES NOT IMPLY** causation !!!

- [Ceteris Paribus VIDEO](https://mru.org/courses/mastering-econometrics/ceteris-paribus)

--

-  **The required assumption to establish causality is the zero conditional mean independence `\(E(u|x)=0\)`**


---

## 2.2 Deriving the Ordinary Least Square (OLS) Estimates

- In order to estimate the regression model one needs data

- A random sample of `\(n\)` observations

&lt;img src="26.png" width="700px" /&gt;

---
### Deriving the Ordinary Least Square (OLS) Estimates

The Ordinary Least Squares method estimates the intercept and slope of a line that “best fits” the observed data by minimizing the sum of the squared distances between the points and the line.

&lt;img src="27.png" width="700px" /&gt;


[**Guess the regression line- SIMULATION**](https://professoramanda.github.io/econsimulations/)

---
### Deriving the Ordinary Least Square (OLS) Estimates (Math)

Let's derive OLS estimates mathematically.

- Define regression **residuals**:

`$$\widehat{u}_{i}=y_{i}-\widehat{y}_{i}=y_{i}-\widehat{\beta}_{0}-\widehat{\beta}_{1} x_{i}$$`

- Minimize the sum of the squared regression residuals 

`$$\min \sum_{i=1}^{n} \widehat{u}_{i}^{2} \rightarrow \widehat{\beta}_{0}, \widehat{\beta}_{1}$$`

- OLS estimators

`$$\widehat{\beta}_{1}=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}$$`



`$$\quad \widehat{\beta}_{0}=\bar{y}-\widehat{\beta}_{1} \bar{x}$$`


---
### Deriving the Ordinary Least Square (OLS) Estimates in R

[Install *wooldridge* package to have access to the data sets-link](https://github.com/JustinMShea/wooldridge)

[Introductory Econometrics Examples-link](https://justinmshea.github.io/wooldridge/articles/Introductory-Econometrics-Examples.html)

Example 2.4 Wage and Education

```r
#install.packages("wooldridge")
library (wooldridge) # need to load the package before using it
library(fixest) # needed to run the regression feols
library(modelsummary)

data ("wage1") # load the data
?wage1 #check out the documentation in the Help panel
# we could use lm package, but feols is useful for future chapters
model&lt;- feols(wage~educ, data=wage1) 
#summary(model)
modelsummary(model)
```
---
Example 2.4 Wage and Education

|            |  Model 1  |
|:-----------|:---------:|
|(Intercept) |  -0.905   |
|            |  (0.685)  |
|educ        |   0.541   |
|            |  (0.053)  |
|Num.Obs.    |    526    |
|R2          |   0.165   |
|R2 Adj.     |   0.163   |
|R2 Within   |           |
|R2 Pseudo   |           |
|AIC         |  2775.4   |
|BIC         |  2784.0   |
|Log.Lik.    | -1385.712 |
|Std.Errors  |    IID    |

---

```r
library(DT)
DT::datatable(head(wage1, 10),
  fillContainer = FALSE, options = list(pageLength = 5))
```

<div id="htmlwidget-ad72900c655b163cbaab" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-ad72900c655b163cbaab">{"x":{"filter":"none","vertical":false,"fillContainer":false,"data":[["1","2","3","4","5","6","7","8","9","10"],[3.09999990463257,3.24000000953674,3,6,5.30000019073486,8.75,11.25,5,3.59999990463257,18.1800003051758],[11,12,11,8,12,16,18,12,12,17],[2,22,2,44,7,9,15,5,26,22],[0,2,0,28,2,8,7,3,4,21],[0,0,0,0,0,0,0,0,0,0],[1,1,0,0,0,0,0,1,1,0],[0,1,0,1,1,1,0,0,0,1],[2,3,2,0,1,0,0,0,2,0],[1,1,0,1,0,1,1,1,1,1],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[1,1,1,1,1,1,1,1,1,1],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,1,0,0,0,1,0,1,0],[0,1,0,0,0,0,0,0,0,0],[0,0,0,0,0,1,0,0,0,0],[0,0,0,0,0,1,1,1,1,1],[0,0,0,1,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0,0],[1.13140213489532,1.17557334899902,1.0986123085022,1.7917594909668,1.66770684719086,2.16905379295349,2.42036819458008,1.60943794250488,1.28093385696411,2.9003221988678],[4,484,4,1936,49,81,225,25,676,484],[0,4,0,784,4,64,49,9,16,441]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>wage<\/th>\n      <th>educ<\/th>\n      <th>exper<\/th>\n      <th>tenure<\/th>\n      <th>nonwhite<\/th>\n      <th>female<\/th>\n      <th>married<\/th>\n      <th>numdep<\/th>\n      <th>smsa<\/th>\n      <th>northcen<\/th>\n      <th>south<\/th>\n      <th>west<\/th>\n      <th>construc<\/th>\n      <th>ndurman<\/th>\n      <th>trcommpu<\/th>\n      <th>trade<\/th>\n      <th>services<\/th>\n      <th>profserv<\/th>\n      <th>profocc<\/th>\n      <th>clerocc<\/th>\n      <th>servocc<\/th>\n      <th>lwage<\/th>\n      <th>expersq<\/th>\n      <th>tenursq<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"columnDefs":[{"className":"dt-right","targets":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

---
## 2.3 Properties of OLS on Any Sample of Data

Fitted (or predicted) values: `\(\hat{y}_i=\hat{\beta}_0+\hat{\beta}_0x_i\)`

Residuals: `\(\hat{u_i}=y_i+\hat{y_i}\)`

Algebraic properties:

  -  Deviation from the regression (residuals) line sum up to zero 

`$$\sum_{i=1}^{n} \widehat{u}_{i}=0$$` 

  -  Covariance between residuals and independent variables is zero 
  
`$$\quad \sum_{i=1}^{n} x_{i} \widehat{u}_{i}=0$$`

   -  Sample averages of `\(y\)` and `\(x\)` lie on the regression line
    
`$$\quad \bar{y}=\widehat{\beta}_{0}+\widehat{\beta}_{1} \bar{x}$$`

---
### CEO data

This table presents the predicted values and residuals for 15 CEOs

  - What is the predicted salary for the `\(12^{th}\)` CEO?
  - Is it higher or lower than their actual salary?
  
  - What is the predicted salary for the `\(5^{th}\)` CEO?
  - Is it higher or lower than their actual salary?
  

---
### Goodness of fit

How well does an independent variable explain the dependent variable?

Total sum of squares (SST)- represents the total variation in the dependent variable

`$$S S T \equiv \sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}$$`

Explained sum of squares (SSE)- represents variation explained by regression

`$$S S E \equiv\sum_{i=1}^{n}\left(\hat{y}_{i}-\bar{y}\right)^{2}$$`
Residual sum of squares (SSR)- represents variation **NOT** explained by regression

`$$\quad S S R \equiv \sum_{i=1}^{n} \hat{u}_{i}{ }^{2}$$`


---
### Goodness of fit

Decomposition of total variation:

Total variation= Explained part + Unexplained part

`$$SST= SSE + SSR$$`

Goodness-of-fit measure (R-squared)

  - R-squared measure the fraction of the total variation that is explained by the regression


`$$R^{2} \equiv \frac{S S E}{S S T}=1-\frac{S S R}{S S T}$$`


---
### Goodness-of-fit measure (R-squared)

&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; salary_hat &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; vote_hat &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 963.191 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 26.812 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; (213.240) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; (0.887) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; roe &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 18.501 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; (11.123) &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; shareA &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.464 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;box-shadow: 0px 1px"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;box-shadow: 0px 1px"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;box-shadow: 0px 1px"&gt; (0.015) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Num.Obs. &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 209 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 173 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; R2 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.013 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.856 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; R2 Adj. &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.008 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.855 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; R2 Within &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; R2 Pseudo &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:center;"&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; AIC &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 3613.1 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1134.4 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; BIC &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 3619.8 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1140.7 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Log.Lik. &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; −1804.543 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; −565.200 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Std.Errors &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; IID &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; IID &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

How do we interpret `\(R^2\)`?

???
The regression explains only 1.3% of the total variation in salaries

The regression explains 85.6% of the total variation in election outcomes



---
## 2.4 Units of measurement and functional form

  - **Level- level regression:**
`\(y=\beta_{0}+\beta_{1} x+\epsilon\)`

*Interpretation: the coefficient `\(\beta_{1}\)` gives us directly the change in `\(Y\)` for a one-unit change in `\(X\)`*

  - **Log- level regression:**  
  
`$$\ln (y)=\beta_{0}+\beta_{1} x+\epsilon$$`

`$$\ln (wage)=\beta_{0}+\beta_{1}educ+\epsilon$$`


`$$\beta_{1}=\frac{\Delta \log (\text { wage })}{\Delta e d u c}=\frac{1}{w a g e} \cdot \frac{\Delta \text { wage }}{\Delta e d u c}=\frac{\frac{\Delta w a g e}{\text { wage }}}{\Delta e d u c}$$`

 `$$\widehat{\ln (wage)}=0,584+0.083~educ+\epsilon$$`
 
 *Interpretation: The wage increases by 8.3% for every additional year of education.*
???
Because the percentage change in wage is the same for each add. year of educ, the change in wage for an extra year of educ increases as education increases.

---
### Incorporating nonlinearities

  - **Level- log regression:**
  
`\(y=\beta_{0}+\beta_{1}ln(x)+\epsilon\)`

*Interpretation: The `\(\frac{\widehat{\beta_{1}}}{100}\)` can be interpreted as the expected increase in `\(Y\)` from a 1% increase in `\(X\)`* 

  - **Log- log regression:**
  
`\(\ln (y)=\beta_{0}+\beta_{1} ln(x)+\epsilon\)`

*Interpretation: The `\(\widehat{\beta_{1}}\)`  is the expected percentage change in `\(Y\)` when `\(X\)` increases by some percentage (elasticity)*

Note: All these models are called linear regression models, even if we allow for nonlinear relationship between `\(x\)` and `\(y\)`

???
use wage and ceo data to estimate and interpret coeff

---
### Incorporating nonlinearities (summary)


![](2a.png)&lt;!-- --&gt;



---
## 2.5 Expected values and variances of the OLS estimators

Population model: `\(y=\beta_{0}+\beta_{1} x+\epsilon\)`

- `\(\widehat{\beta_{0}},\widehat{\beta_{1}}\)` are the estimators for the parameters `\(\beta_{0},\beta_{1}\)`

- `\(\widehat{\beta_{0}},\widehat{\beta_{1}}\)` are random variables with sampling distributions

[Sample Distribution of OLS Estimators- Simulations](https://professoramanda.github.io/econsimulations/)

--
Question: On average, will the `\(\widehat{\beta_{1}}\)` be equal to the true parameter `\(\beta_{1}\)` if  we repeatedly sample? How large will their variability be in repeated samples

---
### Expected values and variances of the OLS estimators

 The estimated regression coefficients are random variables because they are calculated from a random sample:
`$$\widehat{\beta}_{1}=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}, \quad \widehat{\beta}_{0}=\bar{y}-\widehat{\beta}_{1} \bar{x}$$`

Data is random and depends on particular sample that has been drawn.

--

 The question is what the estimators will estimate on average and how large will their variability be in repeated samples
`$$E\left(\widehat{\beta}_{0}\right)=?, E\left(\widehat{\beta}_{1}\right)=? \quad\\
\operatorname{Var}\left(\widehat{\beta}_{0}\right)=?, \operatorname{Var}\left(\widehat{\beta}_{1}\right)=?$$`


---
### Unbiasedness

`$$E(\widehat{\beta_{1}})=\beta_{1}$$` 

--

Unbiasedness means that if we could take as many random samples on `\(Y\)` as we want from the population and compute an estimate each time, the average of the estimates would be equal to `\(\beta_{1}\)`

There are several assumptions required for OLS estimates toe be unbiased


??? We will study properties of the distributions of  `\(\widehat{\beta_{0}},\widehat{\beta_{1}}\)` over different random samples from the population





---
### Standard Assumptions for the Linear Regression Model (SLR- Simple Linear Regression)

- Assumption SLR 1: **Linear in parameters**

`\(y=\beta_{0}+\beta_{1} x+u \longleftarrow \begin{aligned}&amp;\text { In the population, the relationship } \\&amp;\text { between } \mathrm{y} \text { and } \mathrm{x} \text { is linear }\end{aligned}\)`

- Assumption SLR.2 **Random sampling**

`\(\left\{\left(x_{i}, y_{i}\right): i=1, \ldots, n\right\} \leftarrow\)` The data is a random sample drawn from the population

`\(y_{i}=\beta_{0}+\beta_{1} x_{i}+u_{i} \longleftarrow\)` Each data point therefore follows the population equation

&lt;img src="2b.png" width="500px" style="display: block; margin: auto;" /&gt;

???
The population consists, for example, of all workers of country A
In the population, there is a linear relationship between wages (or log wages) and years of education.
Draw completely randomly a worker from the population
The wage and the years of education of the worker drawn are random because one does not know beforehand which worker is drawn.
Throw that worker back into the population and repeat the random draw n times.
The wages and years of education of the sampled workers are used to estimate the linear relationship between wages and education.


---
### Standard Assumptions for the Linear Regression Model (SLR- Simple Linear Regression)

- Assumption SLR.3 **Sample variation in the explanatory variable**

`\(\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}&gt;0\)`


--
The values of the explanatory variables are not all  the same (otherwise it would be impossible to study how different values of the explanatory variable aligned lead to different values of the dependent variable)

- Assumption SLR.4 **Zero conditional mean**

 `\(E\left(u_{i} \mid x_{i}\right)=0\)` 

--

The value of the explanatory variable must contain no information about the mean of the unobserved factors

 
---
### Unbiasedness of the OLS estimates

`\(S L R .1-S L R .4 \Rightarrow E\left(\widehat{\beta}_{0}\right)=\beta_{0}, E\left(\widehat{\beta}_{1}\right)=\beta_{1}\)`

Interpretation of unbiasedness:

- The estimated coefficients may be smaller or larger, depending on the sample that is the result of a random draw.

--

- However, on average, they will be equal to the values that characterize the true relationship between `\(y\)` and `\(x\)` in the population.
--

- "On average" means if sampling was repeated, i.e. if drawing the random sample and doing the estimation was repeated many times.

--

- In a given sample, estimates may differ considerably from true values.


---
### Variances of the OLS estimators

Depending on the sample, the estimates will be nearer or farther away from the true population values.

How far can we expect our estimates to be away from the true population values on average (= sampling variability)?

Sampling variability is measured by the estimator‘s variances



- Assumption SLR5: **Homoskedasticity**

`\(\operatorname{Var}\left(u_{i} \mid x_{i}\right)=\sigma^{2}\)` 

The value of the explanatory variable must contain no information about the variability of the unobserved factors

---
### Homoskedasticity VS Heteroskedasticity


&lt;img src="2c.png" width="600px" style="display: block; margin: auto;" /&gt;

--

&lt;img src="2d.png" width="600px" style="display: block; margin: auto;" /&gt;

---
### Variances of the OLS estimators

`$$\operatorname{Var}\left(\widehat{\beta}_{1}\right)=\frac{\sigma^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}=\frac{\sigma^{2}}{S S T_{x}}$$`



`$$\operatorname{Var}\left(\widehat{\beta}_{0}\right)=\frac{\sigma^{2} n^{-1} \sum_{i=1}^{n} x_{i}^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}=\frac{\sigma^{2} n^{-1} \sum_{i=1}^{n} x_{i}^{2}}{S S T_{x}}$$`

---
### Estimating the error variance

 The variance of u does not depend on `\(x\)` i.e. equal to the unconditional variance
 
`$$\operatorname{Var}\left(u_{i} \mid x_{i}\right)=\sigma^{2}=\operatorname{Var}\left(u_{i}\right)$$`
--

 One could estimate the variance of the errors by calculating the variance of the residuals in the sample; unfortunately this estimate would be biased 
 
`$$\hat{\sigma}^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(\hat{u}_{i}-\overline{\hat{u}}_{i}\right)^{2}=\frac{1}{n} \sum_{i=1}^{n} \hat{u}_{i}^{2}$$`

--

An unbiased estimate of the error variance can be obtained by substracting the number of estimated regression coefficients from the number of observations:

`$$\widehat{\sigma}^{2}=\frac{1}{n-2} \sum_{i=1}^{n} \widehat{u}_{i}^{2}$$`

---
### Unbiasedness of the error variance

`$$S L R .1-S L R .5 \quad \Rightarrow \quad E\left(\widehat{\sigma}^{2}\right)=\sigma^{2}$$`

- Calculation of standard errors for regression coefficients

`$$\begin{aligned}
&amp;\operatorname{se}\left(\widehat{\beta}_{1}\right)=\sqrt{\widehat{\operatorname{Var}}\left(\widehat{\beta}_{1}\right)}=\sqrt{\hat{\sigma}^{2} / S S T_{x}} \\
&amp;\operatorname{se}\left(\hat{\beta}_{0}\right)=\sqrt{\widehat{\operatorname{Var}\left(\hat{\beta}_{0}\right)}}=\sqrt{\hat{\sigma}^{2} n^{-1} \sum_{i=1}^{n} x_{i}^{2} / S S T_{x}}
\end{aligned}$$`

The estimated **standard deviations** of the regression coefficients are called **"standard errors."** They measure how precisely the regression coefficients are estimated.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
