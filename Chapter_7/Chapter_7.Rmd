---
title: "Chapter 7: Multiple Regression Analysis with Qualitative Information"
subtitle: "Introductory Econometrics: A Modern Approach"
#author: "Andra Hiriscau"
#date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
      css: xaringan-themer.css
      #css: ["default", "assets/sydney-fonts.css", "assets/sydney.css"]
      #css: ['example.css']
      lib_dir: libs
      nature:
        slideNumberFormat: "%current%"
        highlightStyle: github
        highlightLines: true
        ratio: 16:9
        countIncrementalSlides: true

---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE) 
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.width = 8, fig.height = 6)
library(tidyverse)
library(gghighlight)
library(jtools)
library (wooldridge) # need to load the package before using it
library(fixest) # needed to run the regression feols
library(modelsummary)
library(magrittr)
library(stargazer)

```



``` {r xaringan-themer, include=FALSE, warning=FALSE}
# install.packages("remotes")
#remotes::install_github('rstudio/chromote')
#remotes::install_github("jhelvy/xaringanBuilder")

library(xaringanBuilder)
library(xaringanthemer)
style_duo_accent(
 # primary_color = "#1381B0",
  primary_color = "#006600", # color first slide and titles
  secondary_color = "#FF961C",
  inverse_header_color = "#FFFFFF"  #white
)

#this to build PDF :)
#build_pdf('https://github.com/andrahiriscau/Econometrics_Slides/blob/main/Lecture_1/Econometrics.html')
#build_pdf('https://andrahiriscau.github.io/Econometrics_Slides/Chapter_2/Chapter_2.html')


library(xaringanBuilder) 
build_pdf("Chapter_7.html")



```


## 7.1 Describing Qualitative Information

Examples: gender, race, industry, region, rating grade...

A way to incorporate qualitative information is to use dummy variables.

They may appear as the dependent or as independent variables

--

A single dummy independent variable

```{r, out.width="500px", fig.align = 'center'}
knitr::include_graphics("7a.png")
```

---
## 7.2 A Single Dummy Independent Varible


$$wage=\beta_{0}+\delta_{0} female +\beta_{1} e d u c+u$$

--

$$\delta_{0}=\mathrm{E}( wage\mid female =1, e d u c)-\mathrm{E}(wage\mid female =0, e d u c)$$

$$\delta_{0}=\mathrm{E}( wage \mid female, educ )-\mathrm{E}( wage \mid male, educ )$$

The key here is that the level of education is the same in both expectations; the difference, $\delta_0$, is due to gender only.


```{r, out.width="500px", fig.align = 'center'}
knitr::include_graphics("7a.png")
```

---
### Dummy variable trap

Why don't we include a dummy variable, say `male`, which is one for males and zero for females?

$wage =\beta_{0}+\gamma_{0} male +\delta_{0} female +\beta_{1} e d u c+u$

--

This model cannot be estimated due to perfect collinearity.

The **dummy variable trap** refers to the problem that not all categories can be included in the regression and one category needs to be left out, which is called a base or reference category.

--

When using dummy variables, one category always has to be omitted:

  - Base category: men

wage $=\beta_{0}+\delta_{0}$ female $+\beta_{1} e d u c+u$

  - Base category: women

wage $=\beta_{0}+\gamma_{0}$ male $+\beta_{1} e d u c+u$


---
Alternatively, one could omit the intercept

$$wage =\gamma_{0} male +\delta_{0} female +\beta_{1} e d u c+u$$


--

Disadvantages:
1. More difficult to test for differences between the parameters $=\gamma_{0}$ and $\delta_{0}$.
2. R-squared formula invalid without an intercept


???

https://www.riinu.me/2014/08/why-does-linear-model-without-an-intercept-forced-through-the-origin-have-a-higher-r-squared-value-calculated-by-r/



---
```{r,echo=TRUE,eval=FALSE}

data(wage1, package='wooldridge')

# Average wage
#lm(wage ~ 1, wage1) %>% summary
#wage1 %>% select(wage) %>% stargazer(type = "text")

# Regression of wage on female
#lm(wage ~ female, wage1) %>% summary

# Graph of wage on female
ggplot(wage1, aes(x = female, y = wage)) +
  theme_bw() +
  geom_point() +
  geom_smooth(aes(col = 'fitted line'), method = "lm", se = F)



```


---
```{r,echo=FALSE,eval=TRUE}

data(wage1, package='wooldridge')

# Average wage
#lm(wage ~ 1, wage1) %>% summary
#wage1 %>% select(wage) %>% stargazer(type = "text")

# Regression of wage on female
#lm(wage ~ female, wage1) %>% summary

# Graph of wage on female
ggplot(wage1, aes(x = female, y = wage)) +
  theme_bw() +
  geom_point() +
  geom_smooth(aes(col = 'fitted line'), method = "lm", se = F)



```



---
```{r,echo=TRUE,eval=FALSE}

data(wage1, package='wooldridge')
#created the dummy variables (male) using the ifelse() function.
# we assign value 0 to male, if female variable is 1; or 
# we assign value 1 to male, if female variable is 0
wage1$male<- ifelse(wage1$female == 1, 0, 1)

# Regression of wage on female
res1<-feols(wage ~ female, wage1)
res2<-feols(wage ~ male, wage1)
res3<-feols(wage ~ male+female-1, wage1)

models<- list(res1,res2,res3)
modelsummary(models,stars = TRUE,fmt = 2,gof_omit = "R2 | R2 Within	|AIC|BIC|Log.Lik.|R2 Pseudo")

```


---
```{r,echo=FALSE,eval=TRUE}

data(wage1, package='wooldridge')
#created the dummy variables (male) using the ifelse() function.
# we assign value 0 to male, if female variable is 1; or 
# we assign value 1 to male, if female variable is 0
wage1$male<- ifelse(wage1$female == 1, 0, 1)

# Regression of wage on female
res1<-feols(wage ~ female, wage1)
res2<-feols(wage ~ male, wage1)
res3<-feols(wage ~ male+female-1, wage1)

models<- list(res1,res2,res3)
modelsummary(models,stars = TRUE,fmt = 2,gof_omit = "R2 | R2 Within	|AIC|BIC|Log.Lik.|R2 Pseudo")

```



---
### Comparing means of subpopulations described by dummies


```{r, out.width="500px", fig.align = 'center'}
knitr::include_graphics("wage2.png")
```

  - Regression 1: Females have $2.51 lower wages than males. The reference category is male.

  - Regression 2: Males have $2.51 higher wages than females. The reference category is female.

  - Regression 1: The intercept or average wage for females is $4.59=\hat{\beta}_{0}+\hat{\delta}_{0}=7.10-2.51$.

  - Regression 2: The intercept or average wage for males is $7.10=\hat{\beta}_{0}+\hat{\delta}_{0}=4.59+2.51$.

  - Regression 3: Both `female` and `male` are included but there is no constant. The coefficients are the **average wage for females and males.**


---
### Example: Effects of training grants on hours of training

```{r, out.width="500px", fig.align = 'center'}
knitr::include_graphics("7d.png")
```

This is an example of program evaluation
  - Treatment group (= grant receivers) vs. control group (= no grant)
  - Is the effect of treatment on the outcome of interest causal?
  
--
    - Zero conditional mean assumption

---
### Using dummy explanatory variables in equations for log(y)


```{r, out.width="500px", fig.align = 'center'}
knitr::include_graphics("7e.png")
```

$\frac{\Delta \log (\text { price })}{\Delta \text { colonial }}=\frac{\% \Delta \text { price }}{\% \Delta \text { colonial }}=5.4 \%$

The house price is 5.4% higher for colonial style houses than the non-colonial houses.

---
## 7.3 Dummy Variables for Multiple Categories

- 1) Define membership in each category by a dummy variable
- 2) Leave out one category (which becomes the base category)


```{r, out.width="800px", fig.align = 'center'}
knitr::include_graphics("marriage.png")
```

---
### Interaction terms

Interaction terms for variables `female` and `married` can be done in two different ways.


  1) Create four categories: `female and single`, `male and single`, `female and married`, and `male and married` and include 3 of them in the regression model (the fourth/omitted category serves as a base/reference category).

$$wage =\beta_{0}+\beta_{1}single female+\beta_{2}married female +\beta_{3}married male+u$$

Reference category: single male

--

  2) Include `female` and `married` and  `female*married` in the regression.

$$wage=\beta_{0}+\beta_{1} female+\beta_{2} married +\beta_{3}female*married +u$$

???

look at the data for the interaction terms


---
### Incorporating ordinal information using dummy variables

Example: City credit ratings and municipal bond interest rates


```{r, out.width="650px", fig.align = 'center'}
knitr::include_graphics("ordinal.png")
```

--

```{r, out.width="650px", fig.align = 'center'}
knitr::include_graphics("ordinal2.png")
```


---
### Interaction terms with indicator variables

```{r,echo=TRUE,eval=FALSE}

data(wage1, package='wooldridge')
# Generate indicator variables
wage1 %<>% mutate(single = 1 - married)
wage1 %<>% mutate(male = 1 - female)
# Categories: female*single, male*single, female*married, male*married
wage1 <-wage1 %>%
                  mutate(female_single  = female*single,
                          male_single  =   male*single,
                          female_married = female*married,
                          male_married =   male*married)

wage1 %>% 
  select(female, male, single, married, female_single, male_single,
         female_married, male_married) %>%
  head(5)
```


---

```{r,echo=FALSE,eval=TRUE}

data(wage1, package='wooldridge')
# Generate indicator variables
wage1 %<>% mutate(single = 1 - married)
wage1 %<>% mutate(male = 1 - female)
# Categories: female*single, male*single, female*married, male*married
wage1 <-wage1 %>%
                  mutate(female_single  = female*single,
                  male_single  =   male*single,
                  female_married = female*married,
                  male_married =   male*married)

wage1 %>% 
  select(female, male, single, married, female_single, male_single,
         female_married, male_married) %>%
  head(5)
```


---

```{r,echo=TRUE,eval=FALSE}

# Regression with male_single as reference category
model_1 <- lm(wage ~ female_single + female_married + male_married, wage1)

# Regression with interaction term
model_2 <- lm(wage ~ female + married + female*married, wage1)

models <- list(model_1, model_2)
modelsummary(models,output = "markdown")

```




---
```{r,echo=FALSE,eval=TRUE}
model_1 <- lm(wage ~ female_single + female_married + male_married, wage1)# Regression with male_single as reference category
model_2 <- lm(wage ~ female + married + female*married, wage1)# Regression with interaction term
models <- list(model_1, model_2)
modelsummary(models,stars = TRUE,fmt = 2, gof_omit = "R2 | R2 Within	|AIC|BIC|Log.Lik.|R2 Pseudo")
```


---
### Incorporating ordinal information using dummy variables

Example: City credit ratings and municipal bond interest rates

```{r, out.width="500px", fig.align = 'center'}
knitr::include_graphics("7g.png")
```

```{r, out.width="500px", fig.align = 'center'}
knitr::include_graphics("7h.png")
```


---
### Interactions involving dummy variables and continuous variables

$$\log (wage )=\beta_{0}+\delta_{0} female +\beta_{1} educ +\delta_{1} female \cdot educ+u$$

$$\begin{array}{ll}\beta_{0}=\text { intercept for men } & \beta_{1}=\text { slope for men } \\\beta_{0}+ \delta_{0}=\text { intercept for women } & \beta_{1}+\delta_{1}=\text { slope for women }\end{array}$$


Interesting hypothesis

```{r, out.width="500px", fig.align = 'center'}
knitr::include_graphics("7i.png")
```

---

### Graphical illustration

```{r, out.width="500px", fig.align = 'center'}
knitr::include_graphics("interaction.png")
```

Interacting both the intercept and the slope with the female dummy enables one to model completely independent wage equations for men and women.



---
### Example: Estimated wage equation with interaction term
 

```{r, out.width="500px", fig.align = 'center'}
knitr::include_graphics("71.png")
```

 - Coefficient on `female`. Does this mean that there is no significant evidence of lower pay for women at the same levels of `educ`, `exper`, and `tenure`? 
 
--

  -  No. This is only the effect for educ = 0. To answer the question one has to recenter the interaction term, e.g. around educ = 12.5 (= average education).

--
  
  - Coefficient on the interaction term `female*educ`, provide no evidence against hypothesis that the return to education is the same for men and women.


---
### Testing for Differences in Regression Functions across Groups


Unrestricted model (contains full set of interactions)

$$cumgpa =\beta_{0}+\delta_{0}female +\beta_{1} sat +\delta_{1} female*sat +\beta_{2} hsperc+\delta_{2} female * h s p e r c+$$
$$+\beta_{3} tothrs +\delta_{3} female* tothrs+u$$


Restricted model (same regression for both groups)

$$cumgpa=\beta_{0}+\beta_{1} sat+\beta_{2} hsperc+\beta_{3} tothrs+u$$
Where:

  - `sat` stadardized aptitude test score
  - `hsperc` high school rank percentile
  - `tothrs` total hours spent in college courses


---

Null hypothesis: All interaction effects are zero i.e. the same regression coefficients apply to men and women

$H_{0}: \delta_{0}=0, \delta_{1}=0, \delta_{2}=0, \delta_{3}=0$


Estimation of the unrestricted model

```{r, out.width="500px", fig.align = 'center'}
knitr::include_graphics("72.png")
```

--

  - Estimation of the restricted model (dropping all female terms) gives us the $R^2$ of .352

  - The `F- statistic` is 8.14 (with a p value of 0.000) which means that we reject the null hypothesis
  
  - Thus, men and women athletes do follow different GPA models





---
### Chow test
Alternative way to compute F-statistic in the given case
 
  - Run separate regressions for men and for women; the unrestricted SSR is given by the sum of the SSR of these two regressions ($SSR_1$ + $SSR_2$).
  - Run regression for the restricted model and store SSR ($SSR_p$ pooled).
  - If the test is computed in this way it is called the Chow-Test.
  - Important: Test assumes a constant error variance across groups (homoskedasticity).


$F=\frac{\left[\mathrm{SSR}_{p}-\left(\mathrm{SSR}_{1}+\mathrm{SSR}_{2}\right)\right]}{\mathrm{SSR}_{1}+\mathrm{SSR}_{2}} \cdot \frac{[n-2(k+1)]}{k+1}$

where `k` represents the number of interaction terms.


---
### Chow test

Joint test with F-statistic (Chow test)


$F=\frac{\left(S S R_{P}-S S R_{u r}\right) / q}{S S R_{u r} /(n-k-1)}=\frac{(85.515-78.355) / 4}{78.355 /(366-7-1)} \approx 8.18$


Null hypothesis is rejected.


---
### 7.5 A Binary Dependent Variable: the Linear Probability Model

$$y=\beta_{0}+\beta_{1} x_{1}+\ldots+\beta_{k} x_{k}+u$$

$$E(y \mid \mathbf{x})=\beta_{0}+\beta_{1} x_{1}+\ldots+\beta_{k} x_{k}$$

If the dependent variable only take on the values 1 and 0
$$E(y \mid \mathbf{x})=1 \cdot P(y=1 \mid \mathbf{x})+0 \cdot P(y=0 \mid \mathbf{x})$$

Linear Probability Model (LPM)
$$P(y=1 \mid \mathbf{x})=\beta_{0}+\beta_{1} x_{1}+\ldots+\beta_{k} x_{k}$$

$$\beta_{j}=\Delta P(y=1 \mid \mathrm{x}) / \Delta x_{j}$$
**In the linear probability model, the coefficients describe the effect of the explanatory variables on the probability that $y=1$**


---
### Example: Labor force participation of married women


```{r, out.width="600px", fig.align = 'center'}
knitr::include_graphics("73.png")
```

---

Graph for `nwifeinc=50`, `exper=5`, `age=30`, `kindslt6=1`, and `kidsge6=0`

```{r, out.width="500px", fig.align = 'center'}
knitr::include_graphics("74.png")
```

The maximum level of education in the sample is educ=17. For the given case, this leads to a predicted probability to be in the labor force of about 50%.


There is a negative predicted probability, but no problem because no woman in the sample has educ < 5.


---

Disadvantages of the linear probability model
  - Predicted probabilities may be larger than one or smaller than zero.
  - Marginal probability effects sometimes logically impossible.
  - The linear probability model is necessarily heteroskedastic.
  - Thus, heteroskedasticity consistent standard errors need to be computed.



$\operatorname{Var}(y \mid \mathbf{x})=P(y=1 \mid \mathbf{x})[1-P(y=1 \mid \mathbf{x})]$

--

Advantages of the linear probability model
  - Easy estimation and interpretation
  - Estimated effects and predictions are often reasonably good in practice.















